# Transformer in Transformer (TNT)
The code will be available as soon. Please create issues if you have any question.

- Paper link: [[arXiv]](https://arxiv.org/abs/2103.00112)
- Official MindSpore code: https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/TNT and pretrained weights on Oxford-IIIT Pets dataset: https://www.mindspore.cn/resources/hub/details?noah-cvlab/gpu/1.1/tnt_v1.0_oxford_pets

- Citation
```
@misc{han2021transformer,
      title={Transformer in Transformer}, 
      author={Kai Han and An Xiao and Enhua Wu and Jianyuan Guo and Chunjing Xu and Yunhe Wang},
      year={2021},
      eprint={2103.00112},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

- Third-party implementations
1. Pytorch: https://github.com/lucidrains/transformer-in-transformer
2. JAX/FLAX: https://github.com/NZ99/transformer_in_transformer_flax
